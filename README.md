------

# ğŸš€ RAP-STOP: Reliable LLM-Powered Multimodal Semantic Top-ğ‘˜ Operator

Modern data lakes hold vast collections of multimodal data, and retrieving the top-ğ‘˜ most relevant items across both structured and unstructured formats is crucial yet difficult. Traditional weighted-sum approaches lack true semantic understanding, and ML-based rankers require extensive labeled dataâ€”human experts remain the gold standard, but are expensive and slow.

**RAP-STOP** (Reliable LAnguage model Powered Semantic Top-ğ‘˜ OPerator) bridges this gap by harnessing LLMs ğŸ¤– for semantic ranking while mitigating their hallucination issues to deliver **reliable**, **efficient**, and **robust** top-ğ‘˜ retrieval.

This repository provides a demo implementation of the associated research paper, demonstrating the recognition and ranking of individualsâ€™ ages based on their images. ğŸ“Š

You can also apply this approach to any other top-k or ranking task you wish! ğŸ‰

------

## ğŸ† Why RAP-STOP?

- **ğŸ” Hallucination Debugging**: Employs Monte Carlo Tree Search (MCTS) to identify and correct structural inconsistencies before ranking.
- **ğŸ¯ Credibility-Aware Ranking**: A learnable attention mechanism dynamically weights evidence, reducing hallucinated noise.
- **âš¡ Enhanced Accuracy**: Achieves over 100% improvement compared to non-learning baselines (PageRank, nDegree, Copeland).
- **â±ï¸ Cost & Time Efficiency**: Delivers accurate Top-k evaluation at under 0.1% of the time and cost of crowd-sourced labeling.
- **ğŸ’° Budget-Friendly LLMs**: Supports free or low-cost models (<$0.14 per million tokens), with seamless integration for premium models.


------

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ sorting_target/           # ğŸ“¸ Original topk target images
â”‚   â”œâ”€â”€ evidenceMatrix.npy        # ğŸ—³ï¸ Original evidence matrix
â”‚   â”œâ”€â”€ evidenceMatrix_debug.npy  # ğŸ› ï¸ Matrix debugged via MCTS
â”‚   â””â”€â”€ realOrder.npy             # âœ”ï¸ Ground-truth ordering matrix
â”œâ”€â”€ E2E_topk_experiment/          # âš™ï¸ End-to-end training and evaluation
â”‚   â”œâ”€â”€ train_topk_operator.py    # ğŸ‹ï¸ Trains the AttentionSorter model
â”‚   â””â”€â”€ evaluate_topk_operator.py # ğŸ“Š Evaluates ranking accuracy
â”œâ”€â”€ MCTS/                         # ğŸŒ² Monte Carlo Tree Search utilities
â”‚   â”œâ”€â”€ mcts/                     # ğŸ§© Core MCTS modules
â”‚   â””â”€â”€ mcts_debug.py             # ğŸ”§ Debugs evidenceMatrix.npy
â”œâ”€â”€ models/                       # ğŸ§  Saved AttentionSorter checkpoints
â””â”€â”€ README.md                     # ğŸ“– This file
```

------

## ğŸ“¦ Requirements

- **Python**: 3.10.16
- **NumPy**: 2.0.1
- **PyTorch**: 2.4.0
- **CUDA Toolkit**: â‰¥11.8
- **Vscode**: optional

------

## ğŸ“¥ Setup & Data Preparation

1. **Prepare Sorting Targets** ğŸ¤³
   - Four images (ages: 20, 27, 69, 85) have been placed into `data/sorting_target/`.
   - We have used LLMs to generate and save `data/evidenceMatrix.npy`.

2. **Provide Ground-Truth Order** âœ…
   - Store the true top-ğ‘˜ order in `data/realOrder.npy` for evaluation.

------

## ğŸ§ª Usage Examples

### 1. Train the Attention Topâ€‘ğ‘˜ Operator
```bash
python .\E2E_topkExperiment\train_topk_operator.py --lr 1e-4 --bs 32 --trainEpochs 100
```
Model checkpoints will be saved in `models/`.

### 2. Debug Evidence Matrix with MCTS
```bash
python .\MCTS\mcts_debug.py --budgetP 0.01 --explorationConstant 1 --epsilon 1e-5 --evidenceLoadPath evidenceMatrix.npy --savePath evidenceMatrix_debug.npy
```
This step resolves hallucination conflicts and outputs `evidenceMatrix_debug.npy`.

### 3. Evaluate Ranking Performance

- **Original Evidence Matrix:**
    ```bash
    python .\E2E_topkExperiment\evaluate_topk_operator.py --evidenceMatrix evidenceMatrix.npy --realOrder realOrder.npy --lr 1e-4 --bs 32 --model_path Visual_K9_M16_N5000_bs32_lr0.0001.pt --result_path evalResult.csv
    ```

- **Debugged Evidence Matrix:**
    ```bash
    python .\E2E_topkExperiment\evaluate_topk_operator.py --evidenceMatrix evidenceMatrix_debug.npy --realOrder realOrder.npy --lr 1e-4 --bs 32 --model_path Visual_K9_M16_N5000_bs32_lr0.0001.pt --result_path evalResult_debug.csv
    ```

------

## ğŸ“ˆ Results & Impact

Using **RAP-STOP**, we demonstrate:
- **ğŸ”¥ Over 100% accuracy improvement** compared to non-learning baselines.
- **âš™ï¸ Seamless integration with existing LLMs** at minimal expense.

------

### ğŸ§™ What it does:

- ğŸ•µï¸ **Hallucination Error Detection**: Identifies and corrects structural inconsistencies in LLM-generated evidence through Monte Carlo Tree Search (MCTS)
- âš–ï¸ **Dynamic Evidence Weighting**: Utilizes a learnable attention mechanism to dynamically weight evidence, minimizing the impact of hallucinated noise
- ğŸŒ **Cross-Modal Ranking**: Handles multimodal data (text/image/numerical and more) through isomorphic partial order representation
- ğŸš€ **Massive Parallel Comparisons**: Executes up to 10^4 pairwise LLM queries per minute via concurrent API calls
- ğŸ§  **Zero-Shot Transfer Learning**: Trains on synthetic data then transfers ranking capability to unseen domains


------
Ready to supercharge your multimodal top-Îº retrieval? Let **RAP-STOP** handle the heavy lifting! âš¡ğŸ”
